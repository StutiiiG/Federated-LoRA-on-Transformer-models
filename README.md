# Federated LoRA: Parameter-Efficient Federated Fine-Tuning of LLMs

> Federated fine-tuning of large language models (LLMs) using LoRA adapters, with heterogeneous clients, Dirichlet data splits, and centralized aggregation of adapter weights only.

[![Python](https://img.shields.io/badge/python-3.10+-blue.svg)]()
[![PyTorch](https://img.shields.io/badge/pytorch-2.x-red.svg)]()
[![License: MIT](https://img.shields.io/badge/license-MIT-green.svg)]()

---

## ðŸ§© Overview

This repository implements a **federated learning framework for LoRA-based fine-tuning of LLMs**.  
Multiple clients fine-tune **only LoRA adapters** on their private data, while a central server **aggregates the adapter weights**, never seeing any raw data.

The default experiments use:

- **Base model:** BLOOM / BLOOMZ or LLaMA-2 (Hugging Face)
- **Task:** Text classification (e.g., SST-2)
- **Federation:** Simulated clients with **Dirichlet data partitioning**
- **PEFT:** [LoRA](https://arxiv.org/abs/2106.09685) with varying ranks (e.g., `r = 16, 64, 256`)

The goal is to explore:

- How well LoRA works in a **federated, non-IID** setting
- Trade-offs between **LoRA rank vs. communication cost vs. accuracy**
- Practical tricks for running federated LLM experiments on **limited GPU budgets**

---

## ðŸ“š Table of Contents

1. [Repository Structure](#-repository-structure)  
2. [Model & Training Architecture](#-model--training-architecture)  
3. [Installation](#-installation)  
4. [Quickstart](#-quickstart)  
5. [Configuration](#-configuration)  
6. [Running Experiments](#-running-experiments)  
7. [Logging & Monitoring](#-logging--monitoring)  
8. [Results](#-results)  
9. [Extending the Project](#-extending-the-project)  
10. [Limitations & Future Work](#-limitations--future-work)  
11. [Citing](#-citing)  
12. [License](#-license)

---

## ðŸ—‚ Repository Structure

```text
federated-lora/
â”œâ”€â”€ configs/
â”‚   â”œâ”€â”€ bloom_sst2_base.yaml
â”‚   â”œâ”€â”€ bloom_sst2_fed.yaml
â”‚   â””â”€â”€ llama2_sst2_fed.yaml
â”œâ”€â”€ data/
â”‚   â””â”€â”€ (downloaded datasets / cached HF datasets)
â”œâ”€â”€ federated/
â”‚   â”œâ”€â”€ client.py           # Client training loop (local LoRA updates)
â”‚   â”œâ”€â”€ server.py           # Aggregation logic (FedAvg / weighted avg)
â”‚   â”œâ”€â”€ partition.py        # Dirichlet data partitioning utilities
â”‚   â””â”€â”€ utils.py
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ lora_wrapper.py     # PEFT / LoRA integration
â”‚   â””â”€â”€ tokenizer_utils.py
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ run_single_lora.py  # Baseline (non-federated) LoRA fine-tuning
â”‚   â”œâ”€â”€ run_federated.py    # Full federated LoRA training script
â”‚   â””â”€â”€ evaluate.py         # Centralized evaluation script
â”œâ”€â”€ notebooks/
â”‚   â””â”€â”€ exploration.ipynb   # EDA & quick checks
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ README.md
â””â”€â”€ LICENSE
